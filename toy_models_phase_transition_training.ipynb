{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGDBMriJ9D_A"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWD1JJVY9CQF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import einops\n",
        "from tqdm.notebook import tqdm\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTEuJsiW_ZFT"
      },
      "source": [
        "# Model definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8q2mkdJL16x"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ModelsConfig:\n",
        "    # Sweep over several models to reduce noise, following the paper\n",
        "    n_models: int = 10\n",
        "    # The paper sweeps over 50 densities and importances\n",
        "    n_densities: int = 50\n",
        "    n_importances: int = 50\n",
        "    # The \"toy model of the toy model\" is a 2D -> 1D -> 2D mapping\n",
        "    d_feature: int = 2\n",
        "    d_model: int = 1\n",
        "    device: str = \"cuda\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyBO4VOO_YWt"
      },
      "outputs": [],
      "source": [
        "class ReLUModels(nn.Module):\n",
        "    def __init__(self, cfg: ModelsConfig):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cfg = cfg\n",
        "        n_models = cfg.n_models\n",
        "        n_densities = cfg.n_densities\n",
        "        n_importances = cfg.n_importances\n",
        "        d_feature = cfg.d_feature\n",
        "        d_model = cfg.d_model\n",
        "        device = cfg.device\n",
        "\n",
        "        self.W = nn.Parameter(torch.empty(n_models, n_densities, n_importances, d_feature, d_model, device=device))\n",
        "        # Kaiming initialization works better than Xavier for layers with ReLU activation\n",
        "        # See https://stats.stackexchange.com/questions/319323/whats-the-difference-between-variance-scaling-initializer-and-xavier-initialize/319849#319849\n",
        "        nn.init.xavier_normal_(self.W)\n",
        "        self.b = nn.Parameter(torch.zeros(n_models, n_densities, n_importances, d_feature, device=device))\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = einops.einsum(\n",
        "            x, self.W,\n",
        "            \"model density importance batch d_feature, model density importance d_feature d_model -> model density importance batch d_model\"\n",
        "        )\n",
        "        out = F.relu(\n",
        "            einops.einsum(\n",
        "                h, self.W,\n",
        "                \"model density importance batch d_model, model density importance d_feature d_model -> model density importance batch d_feature\"\n",
        "            ) + self.b.unsqueeze(-2) # unsqueeze adds batch dimension to b\n",
        "        )\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8KCUk_I_bhS"
      },
      "source": [
        "# Data generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyg96i_m_d1W"
      },
      "outputs": [],
      "source": [
        "def generate_batch(cfg, batch_size, densities, device='cuda'):\n",
        "    n_models = cfg.n_models\n",
        "    n_densities = cfg.n_densities\n",
        "    n_importances = cfg.n_importances\n",
        "    d_feature = cfg.d_feature\n",
        "\n",
        "    feat_vals = torch.rand(n_models, n_densities, n_importances, batch_size, d_feature, device=device)\n",
        "    feat_probs = torch.rand(n_models, n_densities, n_importances, batch_size, d_feature, device=device)\n",
        "    sparsity_mask = feat_probs < densities.view(-1, 1, 1, 1)\n",
        "    # Only those feats with prob < respective density will be present in the generated data\n",
        "    return feat_vals * sparsity_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHy72uPg_fPV"
      },
      "source": [
        "# Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNkblQWT_hhF"
      },
      "outputs": [],
      "source": [
        "class ImportanceWeightedMSELoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, input, target, importances):\n",
        "        squared_error = (target - input) ** 2\n",
        "        # unsqueeze(-2) adds batch dimension to importances before multiplying.\n",
        "        # After adding the new dimension, importances is of shape\n",
        "        # (n_importances, 1, d_feature) and squared_error is of the same shape\n",
        "        # as input, i.e., (..., n_importances, batch_size, d_feature).\n",
        "        # Then, we return the mean error over features and batches.\n",
        "        return einops.reduce(\n",
        "            squared_error * importances.unsqueeze(-2),\n",
        "            \"... batch feature -> ...\",\n",
        "            \"mean\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPkzbUr8_jNE"
      },
      "source": [
        "# Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJIrf560KzVT"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class TrainingArgs:\n",
        "    lr: float = 1e-3\n",
        "    n_epochs: int = 5000\n",
        "    log_interval: int = 150\n",
        "    batch_size: int = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a9Fmfnx_nA2"
      },
      "outputs": [],
      "source": [
        "def train(models: ReLUModels, training_args: TrainingArgs, optimizer: optim.Optimizer, loss_fn: nn.Module, densities, importances, device='cuda'):\n",
        "    optimizer = optimizer(models.parameters(), lr=training_args.lr)\n",
        "    loss_fn = loss_fn()\n",
        "\n",
        "    # Create 1-vs-importance tensor, where 1 represents the importance of the\n",
        "    # first feature relative to the second one\n",
        "    importances = torch.stack((torch.ones(models.cfg.n_importances, device=device), importances), dim=1)\n",
        "\n",
        "    for epoch in tqdm(range(1, training_args.n_epochs + 1)):\n",
        "        batch = generate_batch(models.cfg, training_args.batch_size, densities, device)\n",
        "        # Mean loss over the variables of interest (density and importance) as\n",
        "        # well as different models\n",
        "        loss = (loss_fn(batch, models(batch), importances)).mean()\n",
        "\n",
        "        optimizer.zero_grad() # experiment setting to False\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch [{epoch}/{training_args.n_epochs}]: loss = {loss.item():.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj3xSAcBlziA"
      },
      "source": [
        "# Intializing model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPFiPFVgsbHD"
      },
      "outputs": [],
      "source": [
        "models_cfg = ModelsConfig()\n",
        "\n",
        "models = ReLUModels(models_cfg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEGR7jLlADWi"
      },
      "source": [
        "# Training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wg2I5SHWOe0W"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' # 'cpu' will take >7 hours to train according to early estimates by tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFnxTFRVOMA8"
      },
      "outputs": [],
      "source": [
        "# Both features' density is log-spaced from 0.01 to 1\n",
        "densities = 10 ** torch.linspace(-2, 0, 50, device=device)\n",
        "# Relative importance of the second feature is log-spaced from 0.1 to 10\n",
        "importances = 10 ** torch.linspace(-1, 1, 50, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZ7d1Cn7htX7"
      },
      "outputs": [],
      "source": [
        "# Seed for reproducibility\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKUGjdKolIuU"
      },
      "outputs": [],
      "source": [
        "train(models, TrainingArgs(), optim.AdamW, ImportanceWeightedMSELoss, densities, importances, device)\n",
        "print(\"Training complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6diR4nikZjx",
        "outputId": "ceb119d7-16e5-4f0b-f56c-dacb2853cd9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "save_path = '/content/drive/My Drive/toy-models-superpos/relus-across-densities-and-importances.pth'\n",
        "torch.save(models.state_dict(), save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2--l1Ftgclf"
      },
      "source": [
        "# Plotting results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZspM8WNkoAh",
        "outputId": "261799c1-4f7b-4821-ad10-098ef32dfbfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-8eb3bf511df0>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  models.load_state_dict(torch.load('/content/drive/My Drive/toy-models-superpos/relus-across-densities-and-importances.pth'))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "models.load_state_dict(torch.load('/content/drive/My Drive/toy-models-superpos/relus-across-densities-and-importances.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4VTxz8igeGG"
      },
      "outputs": [],
      "source": [
        "# To do: write the visualize() function\n",
        "visualize(models, densities, importances)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
